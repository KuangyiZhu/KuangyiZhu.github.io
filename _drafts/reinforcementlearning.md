Reinforcement Learning

回报（Return）：
￼
 与 折扣率（discount）
￼
: U 代表执行一组 action 后所有状态累计的 reward 之和，但由于直接的 reward 相加在无限时间序列中会导致无偏向，而且会产生状态的无限循环。因此在这个 Utility 函数里引入 
￼
 折扣率这一概念，令往后的状态所反馈回来的 reward 乘上这个 discount 系数，这样意味着当下的 reward 比未来反馈的 reward 更重要，这也比较符合直觉。定义
￼
由于我们引入了 discount，可以看到我们把一个无限长度的问题转换成了一个拥有最大值上限的问题。
强化学习的目的是最大化长期未来奖励，即寻找最大的 U。（注：回报也作 G 表示）
基于回报（return），我们再引入两个函数
状态价值函数：
￼
，意义为基于 t 时刻的状态 s 能获得的未来回报（return）的期望，加入动作选择策略后可表示为
￼
(
￼
)
动作价值函数：
￼
，意义为基于 t 时刻的状态 s，选择一个 action 后能获得的未来回报（return）的期望
价值函数用来衡量某一状态或动作-状态的优劣，即对智能体来说是否值得选择某一状态或在某一状态下执行某一动作。

https://zhuanlan.zhihu.com/p/25319023


https://www.cnblogs.com/pinard/p/9385570.html
https://www.cnblogs.com/pinard/p/9426283.html

这个递推式子我们一般将它叫做贝尔曼方程。这个式子告诉我们，一个状态的价值由该状态的奖励以及后续状态价值按一定的衰减比例联合组成。

状态价值函数是所有动作价值函数基于策略
𝜋
π
的期望。通俗说就是某状态下所有状态动作价值乘以该动作出现的概率，最后求和，就得到了对应的状态价值
