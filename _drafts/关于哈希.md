### 一般的哈希实现
Java中哈希的实现由两部分构成[3]
* 哈希函数
* 桶
哈希函数由两块组成
* Hash code: h1: keys --> integers
* Compression function: h2: integers --> [0, N-1]

一个好的哈希函数需要满足以下几点
* 均一分布性
* 随机性
* 高效的计算

如何计算哈希函数
* 如果能直接转换为整型则直接使用cast如使用java的```Float.floatToIntBits(x).``` 
* 或者把高位和低位相加
```
static int hashCode(long i) {return (int)((i >>> 32) + (int) i);}

static int hashCode(Double d) {
  long bits = Double.doubleToLongBits(d);
  return (int) (bits ^ (bits >>> 32));
}
```
* 字符串哈希：使用多项式
>Polynomial hash codes: The summation hash code, described above, is not a good choice for character strings or other variable-length objects that can be viewed as a tuple of (x0, x1, ..., xk-1), where the order of xi's is significant. For example, the strings "stop" and "pots" collide using the above hash function. A better hash code should take into account the positions of xi's.

We choose a nonzero constant, a != 1, and calculate (x0ak-1+ x1ak-2+ ...+ xk-2a+ xk-1) as the hash code, ignoring overflows. Mathematically speaking, this is simply a polynomial in a that takes the components (x0, x1, ..., xk-1) of an object x as its coefficients. Since we are more interested in a good spread of the object x with respect to other keys, we simply ignore such overflows.

Experiments have shown that 33, 37, 39, and 41 are particularly good choices for a when working with character strings that are English words. In fact, in a list of over 50,000 English words, taking a to be 33, 37, 39, or 41 produced less than 7 collisions in each case.

Many Java implementations choose the polynomial hash function, using one of these constants for a, as a default hash code for strings. For the sake of speed, however, some Java implementations only apply the polynomial hash function to a fraction of the characters in long strings.

How to evaluate the polynomial? What's the running time? -- By using the Horner's rule. Here is the code performing this evaluation for a string s and a constant a. Your default String.hashCode() uses a = 31 (Ex2.java).

This computation can cause an overflow, especially for long strings. Java ignores these overflows and, for an appropriate choice of a, the result will be a reasonable hash code. The current implementation of the method hashCode in Java's class String uses this computation.

Cyclic shift hash codes: A variant of the polynomial hash code replaces multiplication by a with a cyclic shift of a partial sum by a certain number of bits.  
static int hashCode(String s) {
  int h = 0;
  for (int i = 0; i < s.length(); i++) {
    h = (h << 5) | (h >>> 27); // 5-bit cyclic shift of the running sum
    h += (int) s.charAt(i); // add in next character
  }
  return h;
}
Experiments have been done to calculate the number of collisions over 25,000 English words. It is shown that 5, 6, 7, 9, and 13 are good choices of shift values.

Compression Functions

The hash code for a key k will typically not be suitable for an immediate use with a bucket array since the hash code may be out of bounds. We still need to map the hash code into range [0, N-1]. The goal is to have a compression function that minimizes the possible number of collisions in a given set of hash codes.

The Division method: h2(y) = y mod N.

The size N of the hash table is usually chosen to be a prime number, to help "spread out" the distribution of hash values. For example, think about the hash values {200, 205, 210, 215, 220, ..., 600} with N = 100 or 101. The reason has to do with the number theory and is beyond the scope of this course. Choosing N to be a prime number is not always enough, for if there is a repeated pattern of hash codes of the form pN + q for several different p's, then there will still be collisions.

The MAD method: h2(y) = [(ay + b) mod p] mod N, where N is the size of the hash table, p is a prime number larger than N, and a and b are integers chosen at random from the interval [0, p-1], with a > 0.


As it says, it's a 5-bit cyclic left shift. This means that all the bits are shifted left, with the bit "shifted off" added to the right side, five times.

The code replaces the value of h with the value of two bit patterns ORed together. The first bit pattern is the original value shifted left 5 bits. The second value is the original value shifted right 27 bits.

The left shift of 5 bits puts all the bits but the leftmost five in their final position. The leftmost 5 bits get "shifted out" by that shift and replaced with zeroes as the rightmost bits of the output. The right shift of 27 bits put the leftmost five bits in their final position as the rightmost bits, shifting in zeroes for the leftmost 27 bits. ORing them together produces the desired output.[5]


算法描述：

相关变量：

hash值：一个n位的unsigned int型hash值

offset_basis：初始的哈希值

FNV_prime：FNV用于散列的质数

octet_of_data：8位数据（即一个字节）

FNV-1描述：

hash = offset_basis

for each octet_of_data to be hashed

hash = hash * FNV_prime

hash = hash xor octet_of_data

return hash

FNV-1a描述：

hash = offset_basis 

for each octet_of_data to be hashed

 hash = hash xor octet_of_data

hash = hash * FNV_prime

return hash

FNV-1a和FNV-1的唯一区别就是xor和multiply的顺序不同，他们所采用的FNV_prime和offset_basis都相同，有人认为FNV-1a在进行小数据（小于4个字节）哈希时有更好的性能。


那Java自己的String的hashCode()呢？ 用的是Horner法则
看到了名字很萌很陌陌的MurmurHash，谷歌一看才发现Redis，Memcached，Cassandra，HBase，Lucene都用它。
PS.有些人看到murmur就想到了陌陌就想到了别的，其实是 multiply and rotate的意思，因为算法的核心就是不断的"x *= m; x = rotate_left(x,r);"

定长的哈希函数总是容易搞定的
### extendible hash
Extendible hashing is a type of hash system which treats a hash as a bit string, and uses a trie for bucket lookup.[1] Because of the hierarchical nature of the system, re-hashing is an incremental operation (done one bucket at a time, as needed). This means that time-sensitive applications are less affected by table growth than by standard full-table rehashes.


MDBM has several ways to cope with this:

It can split individual pages in two, using a form of Extendible Hashing.
It has a feature called “overflow pages” that allows some pages to be larger than others.
It has a feature called “large objects” that allows very big single DB entries, which are over a (configurable) size to be placed in a special area in the DB, outside of the normal pages.



### Hash算法的碰撞概率
这个Hash的碰撞问题其实与生日悖论(Birthday problem)一样：

假设从集合m中取n个(n > 0, n <= m)，则取出相同的概率是多少？

16位Hash碰撞概率为50%，则n = 301次
32位Hash碰撞概率为50%，则n = 77162次
128位MD5碰撞概率为50%，则n = 21,719,381,355,163,562,492次

"如果两个字符串在哈希表中对应的位置相同怎么办？",毕竟一个数组容量是有限的，这种可能性很大。解决该问题的方法很多，我首先想到的就是用"链表",感谢大学里学的数据结构教会了这个百试百灵的法宝，我遇到的很多算法都可以转化成链表来解决，只要在哈希表的每个入口挂一个链表，保存所有对应的字符串就OK了。
然而Blizzard的程序员使用的方法则是更精妙的方法。基本原理就是：他们在哈希表中不是用一个哈希值而是用三个哈希值来校验字符串。如果说两个不同的字符串经过一个哈希算法得到的入口点一致有可能，但用三个不同的哈希算法算出的入口点都一致，那几乎可以肯定是不可能的事了，这个几率是1: 18889465931478580854784，大概是10的 22.3次方分之一，对一个游戏程序来说足够安全了。




### References
* [1] : https://stackoverflow.com/questions/4704521/how-to-i-count-key-collisions-when-using-boostunordered-map How to I count key collisions when using boost::unordered_map?
* [2] : https://en.wikipedia.org/wiki/Collision_attack Collision attack
* [3] : https://www.cpp.edu/~ftang/courses/CS240/lectures/hashing.htm CS240 -- Lecture Notes: Hashing
* [4] : http://preshing.com/20110504/hash-collision-probabilities/ Hash Collision Probabilities
* [5] : https://stackoverflow.com/questions/39516565/use-of-and-in-a-hash-function Use of << and >>> in a hash function
* [6] : http://blog.csdn.net/taochenchang/article/details/7319739 FNV哈希算法
* [7] : https://en.wikipedia.org/wiki/MurmurHash MurmurHash
* [8] : http://www.oschina.net/translate/state-of-hash-functions Hash 函数概览 
* [9] : https://www.freezhongzi.info/?p=100 Hash算法的碰撞概率
* [10] : https://www.slideshare.net/mkindika/extendible-hashing Extendible hashing
* [11] : https://yahooeng.tumblr.com/post/104861108931/mdbm-high-speed-database mdbm-high-speed-database
* [12] : https://loonytek.com/2016/05/17/extendible-hashing/ Extendible Hashing
